{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the datasets and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Loading the dataset\n",
    "data = pd.read_csv('youtube_adview.csv')\n",
    "\n",
    "# For Checking the shape and datatypes of the dataset\n",
    "print(data.shape)\n",
    "print(data.dtypes)\n",
    "\n",
    "# Step 2: Visualise the dataset using plotting using heatmaps and plots.\n",
    "\n",
    "f , ax = plt.subplots(figsize = (10,8))\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr,mask = np.zeros_like(corr,dtype = np.bool),cmap = sns.diverging_palette(220,20,as_cmap = True),square = True,ax = ax,annot = True)\n",
    "plt.show()\n",
    "\n",
    "# Assigning each character a number\n",
    "category = {'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'H':8}\n",
    "data['category'] = data['category'].map(category)\n",
    "data['category']\n",
    "\n",
    "data = data[data.views != 'F']\n",
    "data = data[data.likes != 'F']\n",
    "data = data[data.dislikes != 'F']\n",
    "data = data[data.comment ! ='F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clean the dataset by removing missing values and other things.\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Remove any duplicate rows if any\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "data['views'] = pd.to_numeric(data['views'])\n",
    "data['likes'] = pd.to_numeric(data['likes'])\n",
    "data['dislikes'] = pd.to_numeric(data['dislikes'])\n",
    "data['comment'] = pd.to_numeric(data['comment'])\n",
    "data['adview'] = pd.to_numeric(data['adview'])\n",
    "\n",
    "# Step 4: Transform attributes into numerical values and other necessary transformations\n",
    "\n",
    "data['duration'] = LabelEncoder().fit_transform(data['duration'])\n",
    "data['vidid'] = LabelEncoder().fit_transform(data['vidid'])\n",
    "data['published'] = LabelEncoder().fit_transform(data['published'])\n",
    "\n",
    "\n",
    "plt.hist(data['category'])\n",
    "plt.show\n",
    "\n",
    "plt.plot(data['adview'])\n",
    "plt.show\n",
    "\n",
    "data = data[data['adview'] < 2000000]\n",
    "\n",
    "plt.plot(data['adview'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Normalise your data and split the data into training, validation and test set in the appropriate ratio.\n",
    "\n",
    "# Select features and target\n",
    "X = data[['views', 'likes', 'dislikes', 'comment',\n",
    "          'published', 'duration', 'category']]\n",
    "y = data['adview']\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to print Error\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def print_Error(X_test, y_test, model_name):\n",
    "  prediction = model_name.predict(X_test)\n",
    "  print(\"Mean Absolute error of \", model_name,\n",
    "        metrics.mean_absolute_error(y_test, prediction))\n",
    "  print(\"Mean Squared error of \", model_name,\n",
    "        metrics.mean_squared_error(y_test, prediction))\n",
    "  print(\"Root Mean Squared error of \", model_name,np.sqrt(metrics.mean_squared_error(y_test,prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Use linear regression, Support Vector Regressor for training and get errors.\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "linear_reg = LinearRegression()\n",
    "# Train the model\n",
    "linear_reg.fit(X_train, y_train)\n",
    "# Predict on validation set\n",
    "y_pred_val_linear = linear_reg.predict(X_val)\n",
    "# Calculate error\n",
    "error_linear = mean_squared_error(y_val, y_pred_val_linear)\n",
    "print(f'Linear Regression MSE: {error_linear}')\n",
    "print_Error(X_test, y_test, linear_reg)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Initialize the Support Vector Regressor model\n",
    "svr_reg = SVR()\n",
    "# Train the model\n",
    "svr_reg.fit(X_train, y_train)\n",
    "# Predict on validation set\n",
    "y_pred_val_svr = svr_reg.predict(X_val)\n",
    "# Calculate error\n",
    "error_svr = mean_squared_error(y_val, y_pred_val_svr)\n",
    "print(f'SVR MSE: {error_svr}')\n",
    "print_Error(X_test, y_test,svr_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Use Decision Tree Regressor and Random Forest Regressors.\n",
    "\n",
    "# Initialize the Decision Tree Regressor model\n",
    "dt_reg = DecisionTreeRegressor()\n",
    "# Train the model\n",
    "dt_reg.fit(X_train, y_train)\n",
    "# Predict on validation set\n",
    "y_pred_val_dt = dt_reg.predict(X_val)\n",
    "# Calculate error\n",
    "error_dt = mean_squared_error(y_val, y_pred_val_dt)\n",
    "print(f'Decision Tree MSE: {error_dt}')\n",
    "print_Error(X_test, y_test, dt_reg)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Initialize the Random Forest Regressor model\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=25, min_samples_split=15, min_samples_leaf=5)\n",
    "# rf_reg = RandomForestRegressor()\n",
    "# Train the model\n",
    "rf_reg.fit(X_train, y_train)\n",
    "# Predict on validation set\n",
    "y_pred_val_rf = rf_reg.predict(X_val)\n",
    "# Calculate error\n",
    "error_rf = mean_squared_error(y_val, y_pred_val_rf)\n",
    "print(f'Random Forest MSE: {error_rf}')\n",
    "print_Error(X_test, y_test,rf_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Build an artificial neural network and train it with different layers and hyperparameters.\n",
    "\n",
    "# Initialize the ANN model\n",
    "ann = Sequential()\n",
    "ann.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "ann.add(Dense(units=64, activation='relu'))\n",
    "ann.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Compile the ANN\n",
    "ann.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# Train the ANN model\n",
    "ann.fit(X_train, y_train, batch_size=32,\n",
    "        epochs=100, validation_data=(X_val, y_val))\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val_ann = ann.predict(X_val)\n",
    "# Calculate error\n",
    "error_ann = mean_squared_error(y_val, y_pred_val_ann)\n",
    "print(f'ANN MSE: {error_ann}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Pick the best model based on error as well as generalisation.\n",
    "\n",
    "# Compare the errors of all models\n",
    "model_errors = {\n",
    "    'Linear Regression': error_linear,\n",
    "    'SVR': error_svr,\n",
    "    'Decision Tree': error_dt,\n",
    "    'Random Forest': error_rf,\n",
    "    'ANN': error_ann\n",
    "}\n",
    "\n",
    "best_model = min(model_errors, key=model_errors.get)\n",
    "print(f'The best model is: {best_model}')\n",
    "\n",
    "\n",
    "# The test set predictions can now be used for further analysis or creating a submission file, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Save your model and predict on the test set.\n",
    "\n",
    "# Assuming the best model is the Random Forest based on the previous step\n",
    "best_model = rf_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(best_model, 'best_youtube_adview_predictor.pkl')\n",
    "\n",
    "# Load model and predict on the test set (for demonstration)\n",
    "loaded_model = joblib.load('best_youtube_adview_predictor.pkl')\n",
    "test_predictions = loaded_model.predict(X_test)\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.save(\"ann_youtube_adview.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
